{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e058938c",
   "metadata": {},
   "source": [
    "## Finetune QWEN2.5-VL-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e140c338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Sep 13 21:57:36 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.107.02             Driver Version: 550.107.02     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA TITAN Xp                On  |   00000000:02:00.0 Off |                  N/A |\n",
      "| 23%   27C    P8              9W /  250W |       2MiB /  12288MiB |      0%   E. Process |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA TITAN Xp                On  |   00000000:03:00.0 Off |                  N/A |\n",
      "| 23%   34C    P8             16W /  250W |       2MiB /  12288MiB |      0%   E. Process |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01750b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82e313db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Automatically loads .env from the current directory\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "roboflow_api_key = os.getenv(\"ROBOFLOW_API_KEY\")\n",
    "\n",
    "# Optional: Set them in os.environ if downstream libraries expect them there\n",
    "os.environ[\"HF_TOKEN\"] = hf_token\n",
    "os.environ[\"ROBOFLOW_API_KEY\"] = roboflow_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe7e891",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af417d8",
   "metadata": {},
   "source": [
    "### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b182d644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "img_path = \"/swdata/yin/Cui/Re-Veil/create-dataset-new/dataset_zh/zh_char_top3500_rep5_multiFonts202512/zh_char_lower_20250512/labels_train.txt\"\n",
    "# all image names in image_path\n",
    "image_folder = \"/swdata/yin/Cui/Re-Veil/create-dataset-new/dataset_zh/zh_char_top3500_rep5_multiFonts202512/zh_char_lower_20250512/images\"\n",
    "# get the list of all image names in the directory\n",
    "img_list = os.listdir(image_folder)\n",
    "print(f\"Number of images in {image_folder}: {len(img_list)}\")\n",
    "\n",
    "img_csv = pd.read_csv(img_path, sep=\",\")\n",
    "img_dict_train= {}\n",
    "for index, row in img_csv.iterrows():\n",
    "    if row['image_path'] in img_list:\n",
    "        img_dict_train[row['image_path']] = (row['label'], 1) # dummy freq, we don't really need it for val and validation\n",
    "\n",
    "with open(\"/swdata/yin/Cui/EM/reveil/data/zh/zh-char/lower/img_dict_train.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for key, value in img_dict_train.items():\n",
    "        f.write(json.dumps({'image': key, 'prefix': '请识别图片中的汉字', 'label': value[0], 'normal_freq': value[1]}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# print the img_dict\n",
    "# print the first 10 items\n",
    "for i, (key, value) in enumerate(img_dict_train.items()):\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "zh_chars_train = list(img_dict_train.keys())\n",
    "len(zh_chars_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2795903f",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54294a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "# for test dataset, the img items are also stored in annotation file\n",
    "annotation_path = \"/swdata/yin/Cui/EM/reveil/data/zh/zh-char/onestop/char_freqs.csv\"\n",
    "image_folder = \"/swdata/yin/Cui/EM/reveil/data/zh/zh-char/onestop/zh_char_onestop_uniqueChar_newFont/upper/images\"\n",
    "# get the list of all image names in the directory\n",
    "img_list = os.listdir(image_folder)\n",
    "print(f\"Number of images in {image_folder}: {len(img_list)}\")\n",
    "\n",
    "annotation_csv = pd.read_csv(annotation_path, sep=\",\")\n",
    "img_dict_test= {}\n",
    "for index, row in annotation_csv.iterrows():\n",
    "    if row['image_path'] in img_list:\n",
    "        # img_dict_test[row['image_path']] = (row['label'], row['normal_freq'])\n",
    "        img_dict_test[row['image_path']] = (row['label'], row['normal_freq']) # we only need the freq for test set, in baseline model.\n",
    "\n",
    "with open(\"/swdata/yin/Cui/EM/reveil/data/zh/zh-char/onestop/zh_char_onestop_uniqueChar_newFont/upper/img_dict_test.json\", \"w\") as f:\n",
    "    # dump to jsonl format\n",
    "    for key, value in img_dict_test.items():\n",
    "        f.write(json.dumps({'image': key, 'prefix': '请识别图片中的汉字', 'label': value[0], 'normal_freq': value[1]}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "for i, (key, value) in enumerate(img_dict_test.items()):\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "zh_chars_test = list(img_dict_test.keys())\n",
    "len(zh_chars_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60944331",
   "metadata": {},
   "source": [
    "### Vali Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3768d1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"/swdata/yin/Cui/Re-Veil/create-dataset-new/dataset_zh/zh_char_top3500_rep5_multiFonts202512/zh_char_lower_20250512/labels_val.txt\"\n",
    "image_folder = \"/swdata/yin/Cui/Re-Veil/create-dataset-new/dataset_zh/zh_char_top3500_rep5_multiFonts202512/zh_char_lower_20250512/images\"\n",
    "# get the list of all image names in the directory\n",
    "img_list = os.listdir(image_folder)\n",
    "print(f\"Number of images in {image_folder}: {len(img_list)}\")\n",
    "\n",
    "img_csv = pd.read_csv(img_path, sep=\",\")\n",
    "img_dict_val= {}\n",
    "for index, row in img_csv.iterrows():\n",
    "    if row['image_path'] in img_list:\n",
    "        img_dict_val[row['image_path']] = (row['label'], 1) # dummy freq, we don't really need it for val and validation\n",
    "\n",
    "with open(\"/swdata/yin/Cui/EM/reveil/data/zh/zh-char/lower/img_dict_val.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for key, value in img_dict_val.items():\n",
    "        f.write(json.dumps({'image': key, 'prefix': '请识别图片中的汉字', 'label': value[0], 'normal_freq': value[1]}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# print the img_dict\n",
    "# print the first 10 items\n",
    "for i, (key, value) in enumerate(img_dict_val.items()):\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "zh_chars_val = list(img_dict_val.keys())\n",
    "len(zh_chars_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef263d2",
   "metadata": {},
   "source": [
    "### Format data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4b271af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SYSTEM_MESSAGE = \"You are a helpful assistant that can identify Chinese characters in images. The image will show only the upper half of a character, with the lower half masked. Identify the character accurately based on the visible portion.  Please answer with a single character, and do not include any other text.\"\n",
    "# PROMPT = \"The image contains the upper half of a Chinese character. The lower half is masked. What is the character in the image? Please answer with a single word, and do not include any other text. The character is:\"\n",
    "\n",
    "# image_directory_path = \"/swdata/yin/Cui/Re-Veil/create-dataset-new/dataset_zh/zh_char_top3500_rep5_multiFonts202512/zh_char_whole_20250512\"\n",
    "# image_directory_path = \"/swdata/yin/Cui/EM/reveil/data/zh/zh-char/onestop/zh_char_onestop_uniqueChar_newFont/whole/images\"\n",
    "# SYSTEM_MESSAGE = \"你是一个善于识别汉字的智能助手。图片展示了一个汉字，请你准确识别该汉字，你的回答只能包含一个汉字。\"\n",
    "# PROMPT = \"这张图片上有一个汉字。请判断这是什么汉字，只回答一个汉字，不要包含其他内容。这个汉字是：\" \n",
    "\n",
    "# image_directory_path = \"/swdata/yin/Cui/Re-Veil/create-dataset-new/dataset_zh/zh_char_top3500_rep5_multiFonts202512/zh_char_upper_20250512/images\"\n",
    "# image_directory_path = \"/swdata/yin/Cui/EM/reveil/data/zh/zh-char/onestop/zh_char_onestop_uniqueChar_newFont/upper/images\"\n",
    "# SYSTEM_MESSAGE = \"你是一个善于识别汉字的智能助手。图片只展示了一个汉字的上半部分，请你根据上半部分准确识别该汉字，只回答一个汉字。\"\n",
    "# PROMPT = \"这张图片显示的是一个汉字的上半部分，下半部分被遮挡住了。请根据可见部分判断这是什么汉字，只回答一个汉字，不要包含其他内容。这个汉字是：\"\n",
    "\n",
    "# image_directory_path = \"/swdata/yin/Cui/Re-Veil/create-dataset-new/dataset_zh/zh_char_top3500_rep5_multiFonts202512/zh_char_lower_20250512/images\"\n",
    "image_directory_path = \"/swdata/yin/Cui/EM/reveil/data/zh/zh-char/onestop/zh_char_onestop_uniqueChar_newFont/lower/images\"\n",
    "SYSTEM_MESSAGE = \"你是一个善于识别汉字的智能助手。图片只展示了一个汉字的下半部分，请你根据下半部分准确识别该汉字，只回答一个汉字。\"\n",
    "PROMPT = \"这张图片显示的是一个汉字的下半部分，上半部分被遮挡住了。请根据可见部分判断这是什么汉字，只回答一个汉字，不要包含其他内容。这个汉字是：\"\n",
    "\n",
    "def format_data(image_directory_path, entry):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": SYSTEM_MESSAGE}],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": image_directory_path + \"/\" + entry['image'],\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": PROMPT,\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": entry['label']}],\n",
    "        },\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "194feb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class JSONLDataset(Dataset):\n",
    "    def __init__(self, jsonl_file_path: str, image_directory_path: str):\n",
    "        self.jsonl_file_path = jsonl_file_path\n",
    "        self.image_directory_path = image_directory_path\n",
    "        self.entries = self._load_entries()\n",
    "\n",
    "    def _load_entries(self):\n",
    "        entries = []\n",
    "        with open(self.jsonl_file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                data = json.loads(line)\n",
    "                entries.append(data)\n",
    "        # print(entries )\n",
    "        return entries\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        if idx < 0 or idx >= len(self.entries):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "\n",
    "        entry = self.entries[idx]\n",
    "        image_path = os.path.join(self.image_directory_path, entry['image'])\n",
    "        image = Image.open(image_path)\n",
    "        return image, entry, format_data(self.image_directory_path, entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6b572c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_dataset = JSONLDataset(\n",
    "#     jsonl_file_path=\"/swdata/yin/Cui/EM/reveil/data/zh/zh-char/upper/img_dict_train.json\",\n",
    "#     image_directory_path = \"/swdata/yin/Cui/Re-Veil/create-dataset-new/dataset_zh/zh_char_upper_20250512/images\",\n",
    "# )\n",
    "\n",
    "# valid_dataset = JSONLDataset(\n",
    "#     jsonl_file_path = \"/swdata/yin/Cui/EM/reveil/data/zh/zh-char/upper/img_dict_val.json\",\n",
    "#     image_directory_path = \"/swdata/yin/Cui/Re-Veil/create-dataset-new/dataset_zh/zh_char_upper_20250512/images\",\n",
    "# )\n",
    "\n",
    "# test_dataset = JSONLDataset(\n",
    "#     jsonl_file_path = \"/swdata/yin/Cui/EM/reveil/data/zh/zh-char/onestop/zh_char_onestop_uniqueChar_newFont/whole/img_dict_test.json\",\n",
    "#     image_directory_path = \"/swdata/yin/Cui/EM/reveil/data/zh/zh-char/onestop/zh_char_onestop_uniqueChar_newFont/whole/images\",\n",
    "# )\n",
    "\n",
    "# test_dataset = JSONLDataset(\n",
    "#     jsonl_file_path = \"/swdata/yin/Cui/EM/reveil/data/zh/zh-char/onestop/zh_char_onestop_uniqueChar_newFont/upper/img_dict_test.json\",\n",
    "#     image_directory_path = \"/swdata/yin/Cui/EM/reveil/data/zh/zh-char/onestop/zh_char_onestop_uniqueChar_newFont/upper/images\",\n",
    "# )\n",
    "\n",
    "test_dataset = JSONLDataset(\n",
    "    jsonl_file_path = \"/swdata/yin/Cui/EM/reveil/data/zh/zh-char/onestop/zh_char_onestop_uniqueChar_newFont/lower/img_dict_test.json\",\n",
    "    image_directory_path = \"/swdata/yin/Cui/EM/reveil/data/zh/zh-char/onestop/zh_char_onestop_uniqueChar_newFont/lower/images\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cabc203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = [test_dataset2[i] for i in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69a4c3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=54x50>,\n",
       " {'image': '3.jpg', 'prefix': '请识别图片中的汉字', 'label': '上', 'normal_freq': 1},\n",
       " [{'role': 'system',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': '你是一个善于识别汉字的智能助手。图片只展示了一个汉字的下半部分，请你根据下半部分准确识别该汉字，只回答一个汉字。'}]},\n",
       "  {'role': 'user',\n",
       "   'content': [{'type': 'image',\n",
       "     'image': '/swdata/yin/Cui/EM/reveil/data/zh/zh-char/onestop/zh_char_onestop_uniqueChar_newFont/lower/images/3.jpg'},\n",
       "    {'type': 'text',\n",
       "     'text': '这张图片显示的是一个汉字的下半部分，上半部分被遮挡住了。请根据可见部分判断这是什么汉字，只回答一个汉字，不要包含其他内容。这个汉字是：'}]},\n",
       "  {'role': 'assistant', 'content': [{'type': 'text', 'text': '上'}]}])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_dataset[0]\n",
    "\n",
    "test_dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6fb09e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAyADYDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD2kOOm4Mc/KQcBhTlO0k/NuYjo3T6/iAKTkNhh8gJwW4OPqPxoboR9zIJ4/KgAbbghCpJwCePXufSjlc8jgcYP6f8A6qedwCcbQcA+mPp7/wBKRjhslh9QeuefyoAYEULndjJ/h569v8/0pV+YlmA2LgYLD0/z3px+bknBJ2huef8AP+e1NKH77Ecjc3A79cfhmgBEZgoaMgZHJ29e/bPrRSlwr7t4GBjBb/Ee1FACgFkVPlUDvt9vypAWHOCyqMenAz1z9KGMmcdmGMYycE0E5QDbxxlcde2fyoARchhs5ccDLH6DP5U/cGGAQRyBglumARUEzMYHORnaQOc5IHf6dOtef/CbW9T1q31dtSvZ7swzII/MOSMhicfXFAHoyqTIYyAxzuU8cH3peQpALMAM/MM4/H/P+DeEfjO085HbjPf3o2ssZXYVyCOOBj8eaAHqdgAaR1OBwuf/AK/T+eaKSL5wfLAY7jwx7Z60UAC8OScA88dQw/8A1VGcFRk9ASCeNvvRLEs6GIHaSpLEn26/59K8s/4U/dMx2eKbvnkZiPT/AL7/AM8UAeqXXz20xBwdh4ODyAf6V5V8E1LWuugDn7RHg9gcNTj8Hrndt/4Su744bMRGB/330pkXwUkt1Pk+Jp0Dn+C3I3Y7/eoA9XXuzDCdCxxgj/OaA7bSrHkjnJ6+3H+eK8rf4P3iDL+LLnv/AAE/+z1o6B8M7vRdcs9Rk8Q3NwIXJaJ0IBGCOu4+vpQB6GjBCR5e5vcAZHrRSurqp8supyMnJXPWigCRACFyM/5NQOfkkH+wv9aKKAFb7r+xH8zTsfuU/wB//wBmNFFADUJEjYOOn8zRCSYmyc8d/wAaKKAFboPqaKKKAP/Z",
      "image/png": "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",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=54x50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[3][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff84874c",
   "metadata": {},
   "source": [
    "## Zero-Shot Qwen2.5-VL-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42660150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maestro.trainer.models.qwen_2_5_vl.checkpoints import load_model, OptimizationStrategy\n",
    "\n",
    "MODEL_ID_OR_PATH = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "# MODEL_ID_OR_PATH = \"Qwen/Qwen2.5-VL-72B-Instruct\"\n",
    "MIN_PIXELS = 28 * 28\n",
    "MAX_PIXELS = 1028 * 28 * 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d0fe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor, model = load_model(\n",
    "    model_id_or_path=MODEL_ID_OR_PATH,\n",
    "    optimization_strategy=OptimizationStrategy.NONE,\n",
    "    min_pixels=MIN_PIXELS,\n",
    "    max_pixels=MAX_PIXELS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4783ab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2_5_VLProcessor\n",
    "\n",
    "from maestro.trainer.common.utils.device import parse_device_spec\n",
    "from maestro.trainer.models.qwen_2_5_vl.loaders import format_conversation\n",
    "\n",
    "def predict_score_with_inputs(\n",
    "    model: Qwen2_5_VLForConditionalGeneration,\n",
    "    processor: Qwen2_5_VLProcessor,\n",
    "    input_ids: torch.Tensor,\n",
    "    attention_mask: torch.Tensor,\n",
    "    pixel_values: torch.Tensor,\n",
    "    image_grid_thw: torch.Tensor,\n",
    "    device: torch.device,\n",
    "    max_new_tokens: int = 1024,\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Generates predictions from the Qwen2.5-VL model using both textual and visual inputs.\n",
    "\n",
    "    Args:\n",
    "        model (Qwen2_5_VLForConditionalGeneration):\n",
    "            A Qwen2.5-VL model capable of conditional text generation with visual context.\n",
    "        processor (Qwen2_5_VLProcessor):\n",
    "            Preprocessing and postprocessing utility for the Qwen2.5-VL model.\n",
    "        input_ids (torch.Tensor):\n",
    "            Tokenized input text IDs.\n",
    "        attention_mask (torch.Tensor):\n",
    "            Attention mask corresponding to the tokenized input.\n",
    "        pixel_values (torch.Tensor):\n",
    "            Preprocessed image data (pixel values) for visual inputs.\n",
    "        image_grid_thw (torch.Tensor):\n",
    "            Tensor specifying the layout or shape of the provided images.\n",
    "        device (torch.device):\n",
    "            Device on which to run inference (e.g., ``torch.device(\"cuda\")`` or ``torch.device(\"cpu\")``).\n",
    "        max_new_tokens (int):\n",
    "            Maximum number of tokens to generate.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of decoded strings corresponding to the generated sequences.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids.to(device),\n",
    "            attention_mask=attention_mask.to(device),\n",
    "            pixel_values=pixel_values.to(device),\n",
    "            image_grid_thw=image_grid_thw.to(device),\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True,\n",
    "            do_sample=False,  # Non-greedy decoding\n",
    "        )\n",
    "        generated_ids = [\n",
    "            generated_sequence[len(input_sequence) :]\n",
    "            for input_sequence, generated_sequence in zip(input_ids, outputs.sequences)\n",
    "        ]\n",
    "        return processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False), outputs.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e2140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from typing import Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from maestro.trainer.models.qwen_2_5_vl.inference import predict_with_inputs\n",
    "from maestro.trainer.models.qwen_2_5_vl.loaders import format_conversation\n",
    "from maestro.trainer.common.utils.device import parse_device_spec\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def run_qwen_2_5_vl_inference(\n",
    "    model,\n",
    "    processor,\n",
    "    conversation: Union[str, dict],\n",
    "    target_token: str = \"a\",\n",
    "    device: str = \"auto\",\n",
    "    max_new_tokens: int = 1024,\n",
    ") -> Tuple[str, Tuple[int, int]]:\n",
    "    device = parse_device_spec(device)\n",
    "    text = processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, _ = process_vision_info(conversation)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=text,\n",
    "        images=image_inputs,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    responses, scores = predict_score_with_inputs(\n",
    "        **inputs,\n",
    "        model=model,\n",
    "        processor=processor,\n",
    "        device=device,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "    response = responses[0]\n",
    "\n",
    "    logits = scores[0][0]\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    target_token_tokenized = processor.tokenizer.tokenize(target_token) \n",
    "    target_token_id = processor.tokenizer.convert_tokens_to_ids(target_token_tokenized[0])  # a few target tokens will have multiple subtokens\n",
    "    prob_target = probs[target_token_id].item()\n",
    "\n",
    "    # Entropy of the distribution\n",
    "    entropy = -(probs * probs.log()).sum().item()\n",
    "\n",
    "    # Top-5 predictions with probabilities and logits\n",
    "    topk_probs, topk_ids = torch.topk(probs, k=5)\n",
    "    topk_tokens = [processor.tokenizer.decode([idx]).strip() for idx in topk_ids]\n",
    "    top5 = [\n",
    "        {\"token\": tok, \"prob\": prob.item(), \"logit\": logits[idx].item()}\n",
    "        for tok, prob, idx in zip(topk_tokens, topk_probs, topk_ids)\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"target\": target_token,\n",
    "        \"prob_target\": prob_target,\n",
    "        \"entropy\": entropy,\n",
    "        \"top5\": top5\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661b43ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "ent = 0.0\n",
    "acc = 0.0\n",
    "cross_ent = 0.0\n",
    "results = []\n",
    "csv_rows = []\n",
    "json_path = \"/swdata/yin/Cui/EM/reveil/precomputed/ZH/char/onestop/whole/zero_shot_zh_char_whole.json\"\n",
    "csv_path = \"/swdata/yin/Cui/EM/reveil/precomputed/ZH/char/onestop/whole/zero_shot_zh_char_whole.csv\"\n",
    "\n",
    "for image, entry, conversations in test_dataset:\n",
    "    image_id = entry['image']\n",
    "    conversation = conversations[:2]\n",
    "    target = entry['label']\n",
    "    freq = entry['normal_freq']\n",
    "\n",
    "    result = run_qwen_2_5_vl_inference(model=model, processor=processor, conversation = conversation, target_token=target, device=\"cuda\", max_new_tokens=10)\n",
    "    response = result['response']\n",
    "    prob_target = result['prob_target']\n",
    "    entropy = result['entropy']\n",
    "    surp = -np.log(prob_target)\n",
    "    accuracy = int(response.strip() == target.strip())\n",
    "\n",
    "    print(f\"Target_Char: {target}, Response: {result['response']}\")\n",
    "\n",
    "    # ent +=  entropy * freq\n",
    "    ent += entropy\n",
    "    # cross entropy is the negative log likelihood of the target token\n",
    "    cross_ent += surp\n",
    "    print(f\"Entropy: {result['entropy']:.4f}, Cumulative Entropy: {ent:.4f}, Surprisal: {surp:.4f}\")\n",
    "    \n",
    "    # check accuracy\n",
    "    acc += accuracy\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    csv_rows.append([\n",
    "        image_id, target, freq, entropy, surp, prob_target, response, accuracy\n",
    "    ])\n",
    "\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(csv_path, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\n",
    "        \"image_id\", \"target\", \"freq\", \"entropy\", \"surprisal\", \"prob_target\", \"response\", \"acc\"\n",
    "    ])\n",
    "    writer.writerows(csv_rows)\n",
    "\n",
    "# print(f\"Entropy: {ent:.4f}\")\n",
    "print(f\"Entropy: {ent / len(test_dataset):.4f}\")\n",
    "print(f\"Cross Entropy: {cross_ent / len(test_dataset):.4f}\")\n",
    "print(f\"Recognition Accuracy: {acc / len(test_dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05a1693",
   "metadata": {},
   "source": [
    "### Unconditional Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fb9cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(freqs):\n",
    "    total = sum(freqs)\n",
    "    if total == 0:\n",
    "        return 0\n",
    "    return -sum((f / total) * np.log(f / total) for f in freqs if f > 0)\n",
    "# print(\"Entropy of letters:\", entropy([english_letter_freq[letter] for letter in english_letter_freq.keys()]))\n",
    "print(\"Unconditional Entropy of Chinese Characters:\", entropy([test_item[1]['normal_freq'] for test_item in test_dataset]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8497fb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "## re-calculate the entropy by just take their mean since it is conditioned on image input which can be treated as unique.\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path = '/swdata/yin/Cui/EM/reveil/precomputed/ZH/char/onestop/whole/zero_shot_zh_char_whole.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Compute the mean of the \"entropy\" column\n",
    "mean_entropy = df['entropy'].mean()\n",
    "item = len(df)\n",
    "mi = 7.11589- mean_entropy\n",
    "\n",
    "print(f\"Mean entropy: {mean_entropy}\")\n",
    "\n",
    "print(mi)\n",
    "\n",
    "print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5210f8",
   "metadata": {},
   "source": [
    "# Use Finetuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26157de",
   "metadata": {},
   "source": [
    "# Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "522b91a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/swdata/yin/miniconda3/envs/reveil/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 46500.04it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.47it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2_5_VLProcessor\n",
    "\n",
    "MIN_PIXELS = 28 * 28\n",
    "MAX_PIXELS = 1280 * 28 * 28\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"/swdata/yin/Cui/EM/reveil/models/qwen2.5-7b-instruct-reveil-zh-char-lower/best\",\n",
    "    device_map=\"cuda:0\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "processor = Qwen2_5_VLProcessor.from_pretrained(\n",
    "    \"/swdata/yin/Cui/EM/reveil/models/qwen2.5-7b-instruct-reveil-zh-char-lower/best\",\n",
    "    min_pixels=MIN_PIXELS,\n",
    "    max_pixels=MAX_PIXELS\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4df88a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2_5_VLProcessor\n",
    "\n",
    "from maestro.trainer.common.utils.device import parse_device_spec\n",
    "from maestro.trainer.models.qwen_2_5_vl.loaders import format_conversation\n",
    "\n",
    "def predict_score_with_inputs(\n",
    "    model: Qwen2_5_VLForConditionalGeneration,\n",
    "    processor: Qwen2_5_VLProcessor,\n",
    "    input_ids: torch.Tensor,\n",
    "    attention_mask: torch.Tensor,\n",
    "    pixel_values: torch.Tensor,\n",
    "    image_grid_thw: torch.Tensor,\n",
    "    device: torch.device,\n",
    "    max_new_tokens: int = 1024,\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Generates predictions from the Qwen2.5-VL model using both textual and visual inputs.\n",
    "\n",
    "    Args:\n",
    "        model (Qwen2_5_VLForConditionalGeneration):\n",
    "            A Qwen2.5-VL model capable of conditional text generation with visual context.\n",
    "        processor (Qwen2_5_VLProcessor):\n",
    "            Preprocessing and postprocessing utility for the Qwen2.5-VL model.\n",
    "        input_ids (torch.Tensor):\n",
    "            Tokenized input text IDs.\n",
    "        attention_mask (torch.Tensor):\n",
    "            Attention mask corresponding to the tokenized input.\n",
    "        pixel_values (torch.Tensor):\n",
    "            Preprocessed image data (pixel values) for visual inputs.\n",
    "        image_grid_thw (torch.Tensor):\n",
    "            Tensor specifying the layout or shape of the provided images.\n",
    "        device (torch.device):\n",
    "            Device on which to run inference (e.g., ``torch.device(\"cuda\")`` or ``torch.device(\"cpu\")``).\n",
    "        max_new_tokens (int):\n",
    "            Maximum number of tokens to generate.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of decoded strings corresponding to the generated sequences.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids.to(device),\n",
    "            attention_mask=attention_mask.to(device),\n",
    "            pixel_values=pixel_values.to(device),\n",
    "            image_grid_thw=image_grid_thw.to(device),\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True,\n",
    "            do_sample=False,  # Non-greedy decoding\n",
    "        )\n",
    "        generated_ids = [\n",
    "            generated_sequence[len(input_sequence) :]\n",
    "            for input_sequence, generated_sequence in zip(input_ids, outputs.sequences)\n",
    "        ]\n",
    "        return processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False), outputs.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c326b668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from typing import Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from maestro.trainer.models.qwen_2_5_vl.inference import predict_with_inputs\n",
    "from maestro.trainer.models.qwen_2_5_vl.loaders import format_conversation\n",
    "from maestro.trainer.common.utils.device import parse_device_spec\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def run_qwen_2_5_vl_inference(\n",
    "    model,\n",
    "    processor,\n",
    "    conversation: Union[str, dict],\n",
    "    target_token: str = \"a\",\n",
    "    device: str = \"auto\",\n",
    "    max_new_tokens: int = 1024,\n",
    ") -> Tuple[str, Tuple[int, int]]:\n",
    "    device = parse_device_spec(device)\n",
    "    text = processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, _ = process_vision_info(conversation)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=text,\n",
    "        images=image_inputs,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    responses, scores = predict_score_with_inputs(\n",
    "        **inputs,\n",
    "        model=model,\n",
    "        processor=processor,\n",
    "        device=device,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "    response = responses[0]\n",
    "\n",
    "    logits = scores[0][0]\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    target_token_tokenized = processor.tokenizer.tokenize(target_token) \n",
    "    target_token_id = processor.tokenizer.convert_tokens_to_ids(target_token_tokenized[0])  # a few target tokens will have multiple subtokens\n",
    "    prob_target = probs[target_token_id].item()\n",
    "\n",
    "    # Entropy of the distribution\n",
    "    entropy = -(probs * probs.log()).sum().item()\n",
    "\n",
    "    # Top-5 predictions with probabilities and logits\n",
    "    topk_probs, topk_ids = torch.topk(probs, k=5)\n",
    "    topk_tokens = [processor.tokenizer.decode([idx]).strip() for idx in topk_ids]\n",
    "    top5 = [\n",
    "        {\"token\": tok, \"prob\": prob.item(), \"logit\": logits[idx].item()}\n",
    "        for tok, prob, idx in zip(topk_tokens, topk_probs, topk_ids)\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"target\": target_token,\n",
    "        \"prob_target\": prob_target,\n",
    "        \"entropy\": entropy,\n",
    "        \"top5\": top5\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c64c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "ent = 0.0\n",
    "acc = 0.0\n",
    "cross_ent = 0.0\n",
    "results = []\n",
    "csv_rows = []\n",
    "json_path = \"/swdata/yin/Cui/EM/reveil/precomputed/ZH/char/onestop/lower/finetuned_onestop_zh_char_lower_best.json\"\n",
    "csv_path = \"/swdata/yin/Cui/EM/reveil/precomputed/ZH/char/onestop/lower/finetuned_onestop_zh_char_lower_best.csv\"\n",
    "\n",
    "for image, entry, conversations in test_dataset:\n",
    "    image_id = entry['image']\n",
    "    conversation = conversations[:2]\n",
    "    target = entry['label']\n",
    "    freq = entry['normal_freq']\n",
    "\n",
    "    result = run_qwen_2_5_vl_inference(model=model, processor=processor, conversation = conversation, target_token=target, device=\"cuda\", max_new_tokens=1)\n",
    "    response = result['response']\n",
    "    prob_target = result['prob_target']\n",
    "    entropy = result['entropy']\n",
    "    surp = -np.log(prob_target)\n",
    "    accuracy = int(response.strip() == target.strip())\n",
    "\n",
    "    print(f\"Target_Char: {target}, Response: {result['response']}\")\n",
    "\n",
    "    # ent +=  entropy * freq\n",
    "    ent += entropy\n",
    "    # cross entropy is the negative log likelihood of the target token\n",
    "    cross_ent += surp\n",
    "    print(f\"Entropy: {result['entropy']:.4f}, Cumulative Entropy: {ent:.4f}, Surprisal: {surp:.4f}\")\n",
    "    \n",
    "    # check accuracy\n",
    "    acc += accuracy\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    csv_rows.append([\n",
    "        image_id, target, freq, entropy, surp, prob_target, response, accuracy\n",
    "    ])\n",
    "\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(csv_path, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\n",
    "        \"image_id\", \"target\", \"freq\", \"entropy\", \"surprisal\", \"prob_target\", \"response\", \"acc\"\n",
    "    ])\n",
    "    writer.writerows(csv_rows)\n",
    "\n",
    "print(f\"Entropy: {ent / len(test_dataset):.4f}\")\n",
    "print(f\"Cross Entropy: {cross_ent / len(test_dataset):.4f}\")\n",
    "print(f\"Whole Recognition Accuracy: {acc / len(test_dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34bda07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2158b836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from typing import Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from maestro.trainer.models.qwen_2_5_vl.inference import predict_with_inputs\n",
    "from maestro.trainer.models.qwen_2_5_vl.loaders import format_conversation\n",
    "from maestro.trainer.common.utils.device import parse_device_spec\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def run_inference(model, processor, conversation, target_token: str = \"a\", max_new_tokens=1024, device=\"cuda:0\"):\n",
    "    text = processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, _ = process_vision_info(conversation)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, output_scores=True, return_dict_in_generate=True)\n",
    "    generated_ids = outputs.sequences\n",
    "    scores = outputs.scores\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):]\n",
    "        for in_ids, out_ids\n",
    "        in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "\n",
    "    logits = scores[0][0]\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    target_token = processor.tokenizer.tokenize(target_token) \n",
    "    target_token_id = processor.tokenizer.convert_tokens_to_ids(target_token[0])  # a few target tokens will have multiple subtokens\n",
    "    prob_target = probs[target_token_id].item()\n",
    "\n",
    "    # Entropy of the distribution\n",
    "    entropy = -(probs * probs.log()).sum().item()\n",
    "\n",
    "    # Top-5 predictions with probabilities and logits\n",
    "    topk_probs, topk_ids = torch.topk(probs, k=5)\n",
    "    topk_tokens = [processor.tokenizer.decode([idx]).strip() for idx in topk_ids]\n",
    "    top5 = [\n",
    "        {\"token\": tok, \"prob\": prob.item(), \"logit\": logits[idx].item()}\n",
    "        for tok, prob, idx in zip(topk_tokens, topk_probs, topk_ids)\n",
    "    ]\n",
    "    # return output_text[0]\n",
    "    return {\n",
    "        \"response\": output_text[0],\n",
    "        \"target\": target_token,\n",
    "        \"prob_target\": prob_target,\n",
    "        \"entropy\": entropy,\n",
    "        \"top5\": top5\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f30a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, entry, conversation = test_dataset[1]\n",
    "conversation = conversation[:2]\n",
    "suffix = entry[\"label\"]\n",
    "suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_suffix = run_inference(model, processor, conversation, suffix, max_new_tokens=1)\n",
    "generated_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a52165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91f6572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0619b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eb8c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
    "    #     input_ids, attention_mask, pixel_values, image_grid_thw, suffixes = batch\n",
    "    #     print(\"suffixes:\", suffixes)\n",
    "\n",
    "    #     generated_ids = self.model.generate(\n",
    "    #         input_ids=input_ids,\n",
    "    #         attention_mask=attention_mask,\n",
    "    #         pixel_values=pixel_values,\n",
    "    #         image_grid_thw=image_grid_thw,\n",
    "    #         max_new_tokens=10,\n",
    "    #     )\n",
    "    #     # generated_ids_trimmed = [\n",
    "    #     #     out_ids[len(in_ids) :]\n",
    "    #     #     for in_ids, out_ids\n",
    "    #     #     in zip(input_ids, generated_ids)]\n",
    "    #     generated_ids_trimmed = []\n",
    "    #     for in_ids, out_ids in zip(input_ids, generated_ids):\n",
    "    #         print(\"length of in_ids:\", len(in_ids))\n",
    "    #         print(\"length of out_ids:\", len(out_ids))\n",
    "    #         decoded_in_ids = processor.batch_decode(in_ids, skip_special_tokens=True,\n",
    "    #                                                clean_up_tokenization_spaces=False)\n",
    "    #         decoded_out_ids = processor.batch_decode(out_ids, skip_special_tokens=False,\n",
    "    #                                                  clean_up_tokenization_spaces=False)\n",
    "    #         print(\"decoded_in_ids:\", decoded_in_ids)\n",
    "    #         print(\"decoded_out_ids:\", decoded_out_ids)\n",
    "    #         generated_ids_trimmed_single = out_ids[len(in_ids) :]\n",
    "    #         generated_ids_trimmed.append(generated_ids_trimmed_single)\n",
    "    #         print(\"length of generated_ids_trimmed:\", len(generated_ids_trimmed_single))\n",
    "    #         decoded_generated_ids_trimmed = processor.batch_decode(\n",
    "    #             generated_ids_trimmed_single, skip_special_tokens=True,\n",
    "    #             clean_up_tokenization_spaces=False\n",
    "    #         )\n",
    "    #         print(\"decoded_generated_ids_trimmed:\", decoded_generated_ids_trimmed)\n",
    "\n",
    "\n",
    "    #     generated_suffixes = processor.batch_decode(\n",
    "    #         generated_ids_trimmed,\n",
    "    #         skip_special_tokens=True,\n",
    "    #         clean_up_tokenization_spaces=False\n",
    "    #     )\n",
    "\n",
    "    #     # edit_scores = []\n",
    "    #     correct = 0\n",
    "    #     for generated_suffix, suffix in zip(generated_suffixes, suffixes):\n",
    "    #         # Remove any leading or trailing whitespace\n",
    "    #         generated_suffix = generated_suffix.strip()\n",
    "    #         suffix = suffix.strip()\n",
    "    #         if generated_suffix == suffix:\n",
    "    #             correct +=1\n",
    "    #         # score = edit_distance(generated_suffix, suffix)\n",
    "    #         # score = score / max(len(generated_suffix), len(suffix))\n",
    "    #         # edit_scores.append(score)\n",
    "\n",
    "    #         print(\"generated_suffix:\", generated_suffix)\n",
    "    #         print(\"suffix:\", suffix)\n",
    "    #         # print(\"score:\", score)\n",
    "\n",
    "    #     # avg_edit_score = sum(edit_scores) / len(edit_scores)\n",
    "    #     # accuracy = correct / len(suffixes)\n",
    "    #     # self.log(\"val_edit_distance\", avg_edit_score, prog_bar=False, logger=True)\n",
    "    #     assert len(generated_suffixes) == len(suffixes)\n",
    "    #     print(f\"length of generated_suffixes: {len(generated_suffixes)}\")\n",
    "    #     print(f\"length of suffixes: {len(suffixes)}\")\n",
    "    #     print(\"correctness:\", correct)\n",
    "    #     print(\"batch size:\", len(suffixes))\n",
    "\n",
    "\n",
    "    #     # self.log(\"val_correctness\", correct, prog_bar=True, logger=True, batch_size=self.config.get(\"batch_size\"))\n",
    "\n",
    "\n",
    "    #     # return {\"edit_distance\": avg_edit_score, \"accuracy\": accuracy}\n",
    "    #     self.val_outputs.append({\"correctness\": correct, \"batch_size\": len(suffixes)})\n",
    "    \n",
    "    # def on_validation_epoch_end(self):\n",
    "    #     if not self.val_outputs:\n",
    "    #         return\n",
    "\n",
    "    #     # avg_edit = sum(x[\"edit_distance\"] for x in self.val_outputs) / len(self.val_outputs)\n",
    "    #     avg_acc = sum(x[\"correctness\"] for x in self.val_outputs) / sum(x[\"batch_size\"] for x in self.val_outputs)\n",
    "    #     # print(f\"Validation Edit Distance: {avg_edit}\")\n",
    "    #     print(f\"Validation Accuracy: {avg_acc}\")\n",
    "\n",
    "    #     # self.log(\"val_edit_distance_epoch\", avg_edit, prog_bar=False, logger=True)\n",
    "    #     self.log(\"val_accuracy_epoch\", avg_acc, prog_bar=True, logger=True)\n",
    "\n",
    "    #     self.val_outputs.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reveil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
