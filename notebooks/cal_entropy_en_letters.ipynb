{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5c80e07",
   "metadata": {},
   "source": [
    "# Simple Test Script with EN Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e45cd3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import numpy as np\n",
    "import math\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "510e0f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(freqs):\n",
    "    total = sum(freqs)\n",
    "    if total == 0:\n",
    "        return 0\n",
    "    return -sum((f / total) * np.log(f / total) for f in freqs if f > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e2f8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of letters: 2.8944350702720407\n",
      "Entropy of similar letters (upper) --> MI(upper): 2.589331493486497\n",
      "Entropy of similar letters (lower) --> MI(lower): 1.7797182590071035\n"
     ]
    }
   ],
   "source": [
    "## from https://en.wikipedia.org/wiki/Letter_frequency\n",
    "english_letter_freq = {\n",
    "    'a': 0.08167, 'b': 0.01492, 'c': 0.02782, 'd': 0.04253, 'e': 0.12702,\n",
    "    'f': 0.02228, 'g': 0.02015, 'h': 0.06094, 'i': 0.06966, 'j': 0.00153,\n",
    "    'k': 0.00772, 'l': 0.04025, 'm': 0.02406, 'n': 0.06749, 'o': 0.07507,\n",
    "    'p': 0.01929, 'q': 0.00095, 'r': 0.05987, 's': 0.06327, 't': 0.09056,\n",
    "    'u': 0.02758, 'v': 0.00978, 'w': 0.02360, 'x': 0.00150, 'y': 0.01974,\n",
    "    'z': 0.00074\n",
    "}\n",
    "\n",
    "print(\"Entropy of letters:\", entropy([english_letter_freq[letter] for letter in english_letter_freq.keys()]))\n",
    "\n",
    "# annotated groups by 3 colleagues -- upper\n",
    "## losse grouping\n",
    "similar_letters_upper = ['acegoq', 'bh', 'd', 'f', 'ij', 'k', 'l', 'm', 'np', 'r', 's', 't', 'uvwxy', 'z']\n",
    "## tighter grouping\n",
    "similar_letters_upper = ['agq', 'c', 'eo','bh', 'd', 'f', 'ij', 'k', 'l', 'm', 'np', 'r', 's', 't', 'vxy', 'u', 'w','z']\n",
    "\n",
    "# combine the list of similar letters to form a string to check if we have all letters\n",
    "# similar_letters_upper_string = ''.join(similar_letters_upper)\n",
    "# assert(len(similar_letters_upper_string) == 26)\n",
    "\n",
    "# annotated groups by 3 colleagues -- lower\n",
    "## losse grouping\n",
    "similar_letters_lower = ['adu', 'b', 'ce', 'fhilmnr', 'g', 'j', 'kx', 'o', 'p', 'q', 's', 't', 'vw', 'y','z']\n",
    "## tighter grouping\n",
    "similar_letters_lower = ['adu', 'b', 'ce', 'fhilmnrt', 'g', 'j', 'kx', 'o', 'p', 'q', 's', 'vw', 'y','z']\n",
    "# similar_letters_lower_string = ''.join(similar_letters_lower)\n",
    "# assert(len(similar_letters_lower_string) == 26)\n",
    "\n",
    "# Calculate the combined frequency of similar letters and put them in a list\n",
    "similar_letters_freq_upper = []\n",
    "for letters in similar_letters_upper:\n",
    "    freq = 0\n",
    "    for letter in letters:\n",
    "        freq += english_letter_freq[letter]\n",
    "    similar_letters_freq_upper.append(freq)\n",
    "\n",
    "## here, MI(upper) = H(upper) - H(upper|letter ID), and H(upper|letter ID) = 0\n",
    "print(\"Entropy of similar letters (upper) --> MI(upper):\", entropy(similar_letters_freq_upper))\n",
    "\n",
    "# Calculate the combined frequency of similar lower letters and put them in a list\n",
    "similar_letters_freq_lower = []\n",
    "for letters in similar_letters_lower:\n",
    "    freq = 0\n",
    "    for letter in letters:\n",
    "        freq += english_letter_freq[letter]\n",
    "    similar_letters_freq_lower.append(freq)\n",
    "\n",
    "print(\"Entropy of similar letters (lower) --> MI(lower):\", entropy(similar_letters_freq_lower))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6c25e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy H(L) = 2.8944 nats\n",
      "Conditional Entropy H(L|C) = 0.3051 nats\n",
      "Mutual Information I(L; C) = 2.5893 nats\n"
     ]
    }
   ],
   "source": [
    "## If use H(letter ID) - H(letter ID| visible part)) --> sanity check\n",
    "# Entropy H(L) of letters\n",
    "H_L = -sum(p * math.log(p) for p in english_letter_freq.values())\n",
    "\n",
    "# Conditional entropy H(L|O)\n",
    "H_L_given_O = 0.0\n",
    "for cluster in similar_letters_upper:\n",
    "    letters = list(cluster)\n",
    "    cluster_prob = sum(english_letter_freq[l] for l in letters)\n",
    "    if cluster_prob == 0:\n",
    "        continue\n",
    "\n",
    "    cluster_entropy = 0.0\n",
    "    for l in letters:\n",
    "        pl = english_letter_freq[l]\n",
    "        pl_given_o = pl / cluster_prob\n",
    "        cluster_entropy -= pl_given_o * math.log(pl_given_o)\n",
    "\n",
    "    H_L_given_O += cluster_prob * cluster_entropy\n",
    "\n",
    "# Mutual Information\n",
    "I_L_O = H_L - H_L_given_O\n",
    "\n",
    "print(f\"Entropy H(L) = {H_L:.4f} nats\")\n",
    "print(f\"Conditional Entropy H(L|O) = {H_L_given_O:.4f} nats\")\n",
    "print(f\"Mutual Information I(L; O) = {I_L_O:.4f} nats\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4087231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "font_path = '/swdata/yin/Cui/Re-Veil/NotoSansCJK-VF.ttf.ttc'\n",
    "font_size = 220\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "save_dir = \"/swdata/yin/Cui/EM/reveil/data/temp/letters\"\n",
    "\n",
    "# Load font\n",
    "font = ImageFont.truetype(font_path, font_size, index=2)\n",
    "font.set_variation_by_axes([400.0])  # Normal weight\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(f\"{save_dir}/whole\", exist_ok=True)\n",
    "os.makedirs(f\"{save_dir}/upper\", exist_ok=True)\n",
    "os.makedirs(f\"{save_dir}/lower\", exist_ok=True)\n",
    "\n",
    "# Define consistent canvas size based on widest and tallest character\n",
    "max_width = 0\n",
    "max_height = 0\n",
    "for char in alphabet:\n",
    "    dummy_img = Image.new(\"RGB\", (1, 1))\n",
    "    draw = ImageDraw.Draw(dummy_img)\n",
    "    bbox = draw.textbbox((0, 0), char, font=font)\n",
    "    width = bbox[2] - bbox[0]\n",
    "    height = bbox[3] - bbox[1]\n",
    "    max_width = max(max_width, width)\n",
    "    max_height = max(max_height, height)\n",
    "\n",
    "# Add generous padding\n",
    "padding = 40\n",
    "canvas_width = max_width + 2 * padding\n",
    "canvas_height = max_height + 2 * padding\n",
    "\n",
    "for text in alphabet:\n",
    "    # Create blank image\n",
    "    image = Image.new(\"RGB\", (canvas_width, canvas_height), \"white\")\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Compute character's bounding box\n",
    "    bbox = draw.textbbox((0, 0), text, font=font)\n",
    "    text_width = bbox[2] - bbox[0]\n",
    "    text_height = bbox[3] - bbox[1]\n",
    "\n",
    "    # Center the character within the canvas\n",
    "    x = (canvas_width - text_width) // 2 - bbox[0]\n",
    "    y = (canvas_height - text_height) // 2 - bbox[1]\n",
    "\n",
    "    # Draw full letter\n",
    "    draw.text((x, y), text, fill=\"black\", font=font)\n",
    "\n",
    "    # Create upper- and lower-masked versions\n",
    "    image_u = image.copy()\n",
    "    image_l = image.copy()\n",
    "    draw_u = ImageDraw.Draw(image_u)\n",
    "    draw_l = ImageDraw.Draw(image_l)\n",
    "\n",
    "    # Mask lower half\n",
    "    draw_u.rectangle([0, canvas_height // 2, canvas_width, canvas_height], fill=\"white\")\n",
    "    # Mask upper half\n",
    "    draw_l.rectangle([0, 0, canvas_width, canvas_height // 2], fill=\"white\")\n",
    "\n",
    "    # Save all\n",
    "    image.save(f\"{save_dir}/whole/{text}.png\")\n",
    "    image_u.save(f\"{save_dir}/upper/{text}.png\")\n",
    "    image_l.save(f\"{save_dir}/lower/{text}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080a6ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted SSIM scores:\n",
      "n-n: 1.0\n",
      "n-h: 0.9790152060658808\n",
      "n-u: 0.962169018876847\n",
      "n-a: 0.9549007934654298\n",
      "n-q: 0.9530690822585127\n",
      "n-p: 0.9508886732355146\n",
      "n-o: 0.9482252111750056\n",
      "n-c: 0.946247322625113\n",
      "n-b: 0.9452765318750579\n",
      "n-e: 0.9448563864607069\n",
      "n-d: 0.9445567754618669\n",
      "n-x: 0.9397132139774363\n",
      "n-r: 0.9374586405074\n",
      "n-s: 0.9373816377852875\n",
      "n-z: 0.9362951105216054\n",
      "n-w: 0.930590129496269\n",
      "n-i: 0.9279220715328209\n",
      "n-f: 0.9266402154809799\n",
      "n-v: 0.9262825441820689\n",
      "n-l: 0.9245180393065847\n",
      "n-t: 0.9236380948978558\n",
      "n-g: 0.9209390071624466\n",
      "n-y: 0.9156827934423832\n",
      "n-k: 0.9156794387301888\n",
      "n-j: 0.9132833100876664\n",
      "n-m: 0.8928683568253987\n"
     ]
    }
   ],
   "source": [
    "# test perceptual similarity metrics on letter n\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "ssimdict = {}\n",
    "\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "# load two grayscale images of the same size\n",
    "img1 = np.array(Image.open(f\"/swdata/yin/Cui/EM/reveil/data/temp/letters/lower/n.png\").convert(\"L\"))\n",
    "for i in range(26):\n",
    "    img2 = np.array(Image.open(f\"/swdata/yin/Cui/EM/reveil/data/temp/letters/lower/{alphabet[i]}.png\").convert(\"L\"))\n",
    "    if img1.shape != img2.shape:\n",
    "        raise ValueError(\"Images must have the same dimensions.\")\n",
    "    # compute SSIM\n",
    "    score, diff = ssim(img1, img2, full=True)\n",
    "    ssimdict[f\"n-{alphabet[i]}\"] = score\n",
    "\n",
    "# sort the dictionary by SSIM score\n",
    "sorted_ssim = dict(sorted(ssimdict.items(), key=lambda item: item[1], reverse=True))\n",
    "print(\"Sorted SSIM scores:\")\n",
    "for key, value in sorted_ssim.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46479d04",
   "metadata": {},
   "source": [
    "## Baseline -- Letter upper part conditional entropy\n",
    "\n",
    "### As a sanity check, if threshold set to 0, i.e., score > 0.0, calculate the marginal prob etc., the conditional entropy should be the same as, or very similar to the unconditional entropy. This is the case here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed82cbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a|evidence: prob = 0.08753108119694764, entropy = 2.760326905908609\n",
      "b|evidence: prob = 0.01566683817584241, entropy = 2.8034470128141677\n",
      "c|evidence: prob = 0.028506142858606667, entropy = 2.849550294597128\n",
      "d|evidence: prob = 0.04465888925057491, entropy = 2.8034470128141677\n",
      "e|evidence: prob = 0.13337813573026158, entropy = 2.8034470128141677\n",
      "f|evidence: prob = 0.02331299898502653, entropy = 2.8028490786660387\n",
      "g|evidence: prob = 0.022969245149670567, entropy = 2.7438408509979437\n",
      "h|evidence: prob = 0.0639904234876566, entropy = 2.8034470128141677\n",
      "i|evidence: prob = 0.06966069660696608, entropy = 2.8944350702720407\n",
      "j|evidence: prob = 0.001561622862975249, entropy = 2.84929960671849\n",
      "k|evidence: prob = 0.008106433694202641, entropy = 2.8034470128141677\n",
      "l|evidence: prob = 0.04025040250402505, entropy = 2.8944350702720407\n",
      "m|evidence: prob = 0.08349238296838671, entropy = 1.5862270416370545\n",
      "n|evidence: prob = 0.07086829145359277, entropy = 2.8034470128141677\n",
      "o|evidence: prob = 0.08053165697612048, entropy = 2.7593156728606782\n",
      "p|evidence: prob = 0.027864447911249784, entropy = 2.5495185489425993\n",
      "q|evidence: prob = 0.0012262969704010637, entropy = 2.6226335607957663\n",
      "r|evidence: prob = 0.05987059870598707, entropy = 2.8944350702720407\n",
      "s|evidence: prob = 0.06620347602255963, entropy = 2.8028490786660387\n",
      "t|evidence: prob = 0.09243174279152848, entropy = 2.84929960671849\n",
      "u|evidence: prob = 0.028960549389392336, entropy = 2.8034470128141677\n",
      "v|evidence: prob = 0.010269549420894019, entropy = 2.8034470128141677\n",
      "w|evidence: prob = 0.05884112895183005, entropy = 2.0121046635539113\n",
      "x|evidence: prob = 0.0015000150001500017, entropy = 2.8944350702720407\n",
      "y|evidence: prob = 0.02072810895382903, entropy = 2.8034470128141677\n",
      "z|evidence: prob = 0.0007545630671969004, entropy = 2.8542300512952177\n",
      "Overall conditional entropy H(L|E) = 2.7634 nats\n"
     ]
    }
   ],
   "source": [
    "# get perceptual similarity metrics on all letters\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "ssimdict = {}\n",
    "conditional_pob_ent_dict = {}\n",
    "## each target letter has an entropy value\n",
    "conditional_entdict = {}\n",
    "\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "# load two grayscale images of the same size\n",
    "\n",
    "for i in range(26):\n",
    "    target = alphabet[i]\n",
    "    p_marginal = 0\n",
    "    p_similars = []\n",
    "    img1 = np.array(Image.open(f\"/swdata/yin/Cui/EM/reveil/data/temp/letters/upper/{target}.png\").convert(\"L\"))\n",
    "    for j in range(26):\n",
    "        comparison = alphabet[j]\n",
    "        # image loading\n",
    "        img2 = np.array(Image.open(f\"/swdata/yin/Cui/EM/reveil/data/temp/letters/upper/{comparison}.png\").convert(\"L\"))\n",
    "        # ensure same size\n",
    "        if img1.shape != img2.shape:\n",
    "            raise ValueError(\"Images must have the same dimensions.\")\n",
    "        \n",
    "        score, diff = ssim(img1, img2, full=True)\n",
    "        ssimdict[f\"{target}-{comparison}\"] = score\n",
    "\n",
    "        if score > 0.95:    # set to 0 for sanity check\n",
    "            p_marginal += english_letter_freq[comparison]\n",
    "            p_similars.append(english_letter_freq[comparison])\n",
    "\n",
    "    p_target = english_letter_freq[target]\n",
    "    p_target_given_orth = p_target / p_marginal if p_marginal > 0 else 0\n",
    "\n",
    "    # calculate conditional entropy\n",
    "    ent_cond = entropy(p_similars)\n",
    "    conditional_pob_ent_dict[target] = (p_target_given_orth, ent_cond)\n",
    "\n",
    "\n",
    "    print(f\"{target}|evidence: prob = {p_target_given_orth}, entropy = {ent_cond}\")\n",
    "\n",
    "    # calculate overall conditional entropy\n",
    "H_L_given_O = 0.0\n",
    "for letter, (p_target_given_orth, ent_cond) in conditional_pob_ent_dict.items():\n",
    "    p_letter = english_letter_freq[letter]\n",
    "    H_L_given_O += p_letter * ent_cond\n",
    "print(f\"Overall conditional entropy H(L|0) = {H_L_given_O:.4f} nats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ca6281",
   "metadata": {},
   "source": [
    "## Baseline -- Letter lower part conditional entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ea35dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a|evidence: prob = 0.08368428063488162, entropy = 2.849550294597128\n",
      "b|evidence: prob = 0.015467230619311234, entropy = 2.8143918208921375\n",
      "c|evidence: prob = 0.027820278202782035, entropy = 2.8944350702720407\n",
      "d|evidence: prob = 0.045417654471284256, entropy = 2.7420226977207647\n",
      "e|evidence: prob = 0.12963340953625083, entropy = 2.8533066783708536\n",
      "f|evidence: prob = 0.022280222802228026, entropy = 2.8944350702720407\n",
      "g|evidence: prob = 0.029397175536881422, entropy = 2.6499682961028856\n",
      "h|evidence: prob = 0.06254105090311989, entropy = 2.8423151586273367\n",
      "i|evidence: prob = 0.06966069660696608, entropy = 2.8944350702720407\n",
      "j|evidence: prob = 0.0018261025243181952, entropy = 2.639566589032997\n",
      "k|evidence: prob = 0.008485381402506046, entropy = 2.7434905852840115\n",
      "l|evidence: prob = 0.041223281680476044, entropy = 2.8499569429260023\n",
      "m|evidence: prob = 0.04311209862385322, entropy = 2.2112039332731546\n",
      "n|evidence: prob = 0.06915455001895628, entropy = 2.849550294597128\n",
      "o|evidence: prob = 0.07854317939274728, entropy = 2.8069590537580513\n",
      "p|evidence: prob = 0.01976576188866005, entropy = 2.849550294597128\n",
      "q|evidence: prob = 0.0009734304714477474, entropy = 2.849550294597128\n",
      "r|evidence: prob = 0.05987059870598707, entropy = 2.8944350702720407\n",
      "s|evidence: prob = 0.0645717668190725, entropy = 2.8533066783708536\n",
      "t|evidence: prob = 0.09056090560905611, entropy = 2.8944350702720407\n",
      "u|evidence: prob = 0.02826022358160934, entropy = 2.849550294597128\n",
      "v|evidence: prob = 0.010376217454962125, entropy = 2.8026499802001124\n",
      "w|evidence: prob = 0.026620344259706278, entropy = 2.667718644953442\n",
      "x|evidence: prob = 0.0015000150001500017, entropy = 2.8944350702720407\n",
      "y|evidence: prob = 0.021138072087893263, entropy = 2.7925012773632183\n",
      "z|evidence: prob = 0.0007728782403442442, entropy = 2.8392869144337145\n",
      "Overall conditional entropy H(L|E) = 2.8273 nats\n"
     ]
    }
   ],
   "source": [
    "# test perceptual similarity metrics on letters\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "ssimdict = {}\n",
    "conditional_pob_ent_dict = {}\n",
    "## each target letter has an entropy value\n",
    "conditional_entdict = {}\n",
    "\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "# Load two grayscale images of the same size\n",
    "\n",
    "for i in range(26):\n",
    "    target = alphabet[i]\n",
    "    p_marginal = 0\n",
    "    p_similars = []\n",
    "    img1 = np.array(Image.open(f\"/swdata/yin/Cui/EM/reveil/data/temp/letters/lower/{target}.png\").convert(\"L\"))\n",
    "    for j in range(26):\n",
    "        comparison = alphabet[j]\n",
    "        img2 = np.array(Image.open(f\"/swdata/yin/Cui/EM/reveil/data/temp/letters/lower/{comparison}.png\").convert(\"L\"))\n",
    "        if img1.shape != img2.shape:\n",
    "            raise ValueError(\"Images must have the same dimensions.\")\n",
    "        \n",
    "        score, diff = ssim(img1, img2, full=True)\n",
    "        ssimdict[f\"{target}-{comparison}\"] = score\n",
    "\n",
    "        if score > 0.95:\n",
    "            p_marginal += english_letter_freq[comparison]\n",
    "            p_similars.append(english_letter_freq[comparison])\n",
    "\n",
    "    p_target = english_letter_freq[target]\n",
    "    p_target_given_orth = p_target / p_marginal if p_marginal > 0 else 0\n",
    "\n",
    "    # calculate conditional entropy\n",
    "    ent_cond = entropy(p_similars)\n",
    "    conditional_pob_ent_dict[target] = (p_target_given_orth, ent_cond)\n",
    "\n",
    "\n",
    "    print(f\"{target}|evidence: prob = {p_target_given_orth}, entropy = {ent_cond}\")\n",
    "\n",
    "    # calculate overall conditional entropy\n",
    "H_L_given_O = 0.0\n",
    "for letter, (p_target_given_orth, ent_cond) in conditional_pob_ent_dict.items():\n",
    "    p_letter = english_letter_freq[letter]\n",
    "    H_L_given_O += p_letter * ent_cond\n",
    "print(f\"Overall conditional entropy H(L|0) = {H_L_given_O:.4f} nats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab3518c",
   "metadata": {},
   "source": [
    "## Baseline -- Letter whole part conditional entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de73b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a|evidence: prob = 0.18473196109477497, entropy = 1.7096349315395556\n",
      "b|evidence: prob = 0.19667809122066965, entropy = 0.49576265101622263\n",
      "c|evidence: prob = 0.08035585338378441, entropy = 1.5164366746465803\n",
      "d|evidence: prob = 1.0, entropy = -0.0\n",
      "e|evidence: prob = 0.287154677397477, entropy = 1.7102104580214696\n",
      "f|evidence: prob = 0.07883376972613404, entropy = 1.516467418338439\n",
      "g|evidence: prob = 1.0, entropy = -0.0\n",
      "h|evidence: prob = 0.4251133589117545, entropy = 0.9537963577549367\n",
      "i|evidence: prob = 0.24515220834066512, entropy = 1.5418017739397445\n",
      "j|evidence: prob = 0.013729361091170135, entropy = 0.7203972868810202\n",
      "k|evidence: prob = 1.0, entropy = -0.0\n",
      "l|evidence: prob = 0.14165053668836883, entropy = 1.5418017739397445\n",
      "m|evidence: prob = 1.0, entropy = -0.0\n",
      "n|evidence: prob = 0.1443358497829295, entropy = 1.8322493701467113\n",
      "o|evidence: prob = 0.16980321194299933, entropy = 1.7096349315395556\n",
      "p|evidence: prob = 1.0, entropy = -0.0\n",
      "q|evidence: prob = 1.0, entropy = -0.0\n",
      "r|evidence: prob = 0.15924990025269317, entropy = 1.8813991356183657\n",
      "s|evidence: prob = 0.14529463096495662, entropy = 1.7159912186485549\n",
      "t|evidence: prob = 0.32043025971268846, entropy = 1.516467418338439\n",
      "u|evidence: prob = 0.10952702434375122, entropy = 1.3211248973794307\n",
      "v|evidence: prob = 1.0, entropy = -0.0\n",
      "w|evidence: prob = 1.0, entropy = -0.0\n",
      "x|evidence: prob = 0.02415070037031074, entropy = 0.1781111642125585\n",
      "y|evidence: prob = 1.0, entropy = -0.0\n",
      "z|evidence: prob = 0.004830287206266318, entropy = 1.1132511049393115\n",
      "Overall conditional entropy H(L|E) = 1.3170 nats\n"
     ]
    }
   ],
   "source": [
    "# perceptual similarity metrics on all letters\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "ssimdict = {}\n",
    "conditional_pob_ent_dict = {}\n",
    "## each target letter has an entropy value\n",
    "conditional_entdict = {}\n",
    "\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "for i in range(26):\n",
    "    target = alphabet[i]\n",
    "    p_marginal = 0\n",
    "    p_similars = []\n",
    "    img1 = np.array(Image.open(f\"/swdata/yin/Cui/EM/reveil/data/temp/letters/whole/{target}.png\").convert(\"L\"))\n",
    "    for j in range(26):\n",
    "        comparison = alphabet[j]\n",
    "        img2 = np.array(Image.open(f\"/swdata/yin/Cui/EM/reveil/data/temp/letters/whole/{comparison}.png\").convert(\"L\"))\n",
    "        if img1.shape != img2.shape:\n",
    "            raise ValueError(\"Images must have the same dimensions.\")\n",
    "\n",
    "        score, diff = ssim(img1, img2, full=True)\n",
    "        ssimdict[f\"{target}-{comparison}\"] = score\n",
    "\n",
    "        if score > 0.9:\n",
    "            p_marginal += english_letter_freq[comparison]\n",
    "            p_similars.append(english_letter_freq[comparison])\n",
    "\n",
    "    p_target = english_letter_freq[target]\n",
    "    p_target_given_orth = p_target / p_marginal if p_marginal > 0 else 0\n",
    "\n",
    "    # calculate conditional entropy\n",
    "    ent_cond = entropy(p_similars)\n",
    "    conditional_pob_ent_dict[target] = (p_target_given_orth, ent_cond)\n",
    "\n",
    "\n",
    "    print(f\"{target}|evidence: prob = {p_target_given_orth}, entropy = {ent_cond}\")\n",
    "\n",
    "    # calculate overall conditional entropy\n",
    "H_L_given_O = 0.0\n",
    "for letter, (p_target_given_orth, ent_cond) in conditional_pob_ent_dict.items():\n",
    "    p_letter = english_letter_freq[letter]\n",
    "    H_L_given_O += p_letter * ent_cond\n",
    "print(f\"Overall conditional entropy H(L|0) = {H_L_given_O:.4f} nats\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reveil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
