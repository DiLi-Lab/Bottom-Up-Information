filter(question_id != -99) %>%
distinct() %>%
mutate(rev = factor(rev, levels = c("f", "u", "l"), labels = c("Full", "Upper", "Lower")),
easiness = factor(easiness, levels = c("equal", "top", "bottom"), labels = c("Equal", "Upper", "Lower")),
subj_id = as.numeric(factor(subj, levels = unique(subj)))
)
zhen_rcq <- rbind(zh_rcq, en_rcq)
zhen_rcq <- zhen_rcq %>%
mutate(msubj_id = as.numeric(factor(subj, levels = unique(subj))),
expr_id = factor(expr_id, levels = c("zh1", "en1"), labels = c("ZH", "EN")))
rcqsum <- zhen_rcq %>%
group_by(expr_id, rev) %>%
summarise(
mean_correctness = mean(correctness, na.rm = TRUE),
n = n(),
.groups = "drop"
) %>%
mutate(
label = paste0(round(mean_correctness * 100), "%")
)
rcqsum
subjrcq <- zhen_rcq %>%
group_by(expr_id, stim_id, question_id, rev) %>%
summarise(mean_correctness = mean(correctness, na.rm = TRUE), .groups = "drop") %>%
pivot_wider(names_from = rev, values_from = mean_correctness) %>%
drop_na()
subjrcq
zh_subjrcq <- subjrcq %>%
filter(expr_id == "ZH")
en_subjrcq <- subjrcq %>%
filter(expr_id == "EN")
zht_Whole_Upper <- t.test(zh_subjrcq$Full, zh_subjrcq$Upper, paired = TRUE)
ent_Whole_Upper <- t.test(en_subjrcq$Full, en_subjrcq$Upper, paired = TRUE)
zht_Whole_Lower <- t.test(zh_subjrcq$Full, zh_subjrcq$Lower, paired = TRUE)
ent_Whole_Lower <- t.test(en_subjrcq$Full, en_subjrcq$Lower, paired = TRUE)
zht_Upper_Lower <- t.test(zh_subjrcq$Upper, zh_subjrcq$Lower, paired = TRUE)
ent_Upper_Lower <- t.test(en_subjrcq$Upper, en_subjrcq$Lower, paired = TRUE)
list(
zh_Whole_vs_Upper = zht_Whole_Upper,
en_Whole_vs_Upper = ent_Whole_Upper,
zh_Whole_vs_Lower = zht_Whole_Lower,
en_Whole_vs_Lower = ent_Whole_Lower,
zh_Upper_vs_Lower = zht_Upper_Lower,
en_Upper_vs_Lower = ent_Upper_Lower
)
easysum <- zhen_rcq %>%
dplyr::select(expr_id, subj, easiness) %>%
distinct() %>%
mutate(easiness = factor(easiness, levels = c("Equal", "Upper", "Lower")),
expr_id = factor(expr_id, levels = c("ZH", "EN"), labels=c("ZH", "EN"))) %>%
count(expr_id, easiness) %>%
group_by(expr_id) %>%
mutate(
easiness_prop = n / sum(n),
label = paste0(round(easiness_prop * 100), "%"),
ypos = case_when(
easiness == "Upper" ~ 0.50,
easiness == "Equal" ~ if_else(expr_id == "EN", 0.96, 0.94),
easiness == "Lower" ~ if_else(expr_id == "EN", 0.02, 0.08),
TRUE ~ NA_real_
)
)
easysum
label_easysum <- easysum %>%
group_by(expr_id) %>%
summarise(n = sum(n), .groups = "drop") %>%
mutate(
center_label = paste0(expr_id, "\n (n=", n, ")"),
x = 0.5,
y = 0.25
)
ggplot(easysum, aes(x = 2, y = easiness_prop, fill = easiness)) +
geom_col(width = 1, color = "white") +
coord_polar(theta = "y") +
geom_text(aes(y = ypos, label = label), color = "black", size = 4) +
geom_text(
data = label_easysum,
aes(x = x, y = y, label = center_label),
inherit.aes = FALSE,
size = 5,
fontface = "bold"
) +
xlim(0.5, 2.5) +
facet_wrap(~ expr_id) +
# facet_grid(expr_id ~ .) +   # vertical alignment
scale_fill_brewer(palette = "Set2") +
theme_void() +
labs(
# title = "Perceived Readability: Upper vs. Lower Word Parts",
fill = "Readability"
) +
theme(
plot.title = element_text(hjust = 0.5, size = 12, face = "bold"),
strip.text = element_blank(),
panel.spacing = unit(0.1, "lines")
)
# ggsave("./visualization/readability_comparison_donut.pdf", width = 4, height = 8)
ggplot(easysum, aes(x = 2, y = easiness_prop, fill = easiness)) +
geom_col(width = 1, color = "white") +
coord_polar(theta = "y") +
geom_text(aes(y = ypos, label = label), color = "black", size = 4) +
geom_text(
data = label_easysum,
aes(x = x, y = y, label = center_label),
inherit.aes = FALSE,
size = 5,
fontface = "bold"
) +
xlim(0.5, 2.5) +
facet_wrap(~ expr_id) +
scale_fill_manual(
values = c(
"Upper" = "#FC8D62",   # orange
"Lower" = "#8DA0CB",   # blue
"Equal" = "#B3B3B3"    # neutral gray
),
labels = c(
"Upper" = "Upper easier than Lower",
"Lower" = "Lower easier than Upper",
"Equal" = "Upper & Lower equally easy"
)
) +
theme_void() +
labs(
fill = "Self-rated ease of reading"
) +
theme(
plot.title = element_text(hjust = 0.5, size = 12, face = "bold"),
strip.text = element_blank(),
panel.spacing = unit(0.1, "lines")
)
# ggsave("./visualization/readability_comparison_donut_pretty.pdf", width = 7, height = 4)
subj_rt <- zhen_rt %>%
mutate(expr_id = factor(expr_id, levels = c("ZH", "EN"), labels=c("ZH", "EN")),
rev = factor(rev, levels = c("Full", "Upper", "Lower"), labels=c("Full", "Upper", "Lower"))) %>%
################# if want to filter out outliers whose rt is 3sd away from the mean #################
#   group_by(expr_id, rev) %>%
# group_modify(~ {
#   df <- .
#   df %>%
#     filter(
#       between(first_duration,
#               mean(first_duration, na.rm = TRUE) - 3 * sd(first_duration, na.rm = TRUE),
#               mean(first_duration, na.rm = TRUE) + 3 * sd(first_duration, na.rm = TRUE)),
#       between(total_duration,
#               mean(total_duration, na.rm = TRUE) - 3 * sd(total_duration, na.rm = TRUE),
#               mean(total_duration, na.rm = TRUE) + 3 * sd(total_duration, na.rm = TRUE)),
#       between(gaze_duration,
#               mean(gaze_duration, na.rm = TRUE) - 3 * sd(gaze_duration, na.rm = TRUE),
#               mean(gaze_duration, na.rm = TRUE) + 3 * sd(gaze_duration, na.rm = TRUE)),
#       between(go_past_time,
#               mean(go_past_time, na.rm = TRUE) - 3 * sd(go_past_time, na.rm = TRUE),
#               mean(go_past_time, na.rm = TRUE) + 3 * sd(go_past_time, na.rm = TRUE))
#     )
# }) %>%
#############################################################################################
group_by(expr_id, subj, rev) %>%
summarise(mean_fd = mean(first_duration, na.rm = TRUE),
mean_td = mean(total_duration, na.rm = TRUE),
mean_gd = mean(gaze_duration, na.rm = TRUE),
mean_gpt = mean(go_past_time, na.rm = TRUE),
mean_fpfix = mean(FPFix, na.rm = TRUE),
mean_fpreg = mean(FPReg, na.rm = TRUE),
mean_regin = mean(RegIn_incl, na.rm = TRUE),
mean_nfix = mean(NFix, na.rm = TRUE),
mean_skip = mean(skip, na.rm = TRUE),
.groups = "drop") %>%
pivot_wider(names_from = rev, values_from = c(mean_fd, mean_td, mean_gd, mean_gpt, mean_fpfix, mean_fpreg, mean_regin, mean_nfix, mean_skip))
subj_rt
measures <- c("mean_fd", "mean_td", "mean_gd", "mean_gpt",
"mean_fpfix", "mean_fpreg", "mean_regin", "mean_nfix", "mean_skip")
run_pairwise_t <- function(df, prefix) {
list(
upper_vs_whole = t.test(df[[paste0(prefix, "_Upper")]], df[[paste0(prefix, "_Full")]], paired = TRUE),
upper_vs_lower = t.test(df[[paste0(prefix, "_Upper")]], df[[paste0(prefix, "_Lower")]], paired = TRUE),
whole_vs_lower = t.test(df[[paste0(prefix, "_Full")]], df[[paste0(prefix, "_Lower")]], paired = TRUE)
)
}
temp_zh <- subj_rt %>%
filter(expr_id == "ZH")
temp_en <- subj_rt %>%
filter(expr_id == "EN")
# Apply the t-tests and extract p-values
t_test_results_zh <- lapply(measures, function(m) run_pairwise_t(temp_zh, m))
t_test_results_en <- lapply(measures, function(m) run_pairwise_t(temp_en, m))
names(t_test_results_zh) <- measures
names(t_test_results_en) <- measures
t_test_results_zh
t_test_results_en
set.seed(123)
subj_gd_long <- subj_rt %>%
dplyr::select(expr_id, subj, starts_with("mean_gd_")) %>%
pivot_longer(
cols = starts_with("mean_gd_"),
names_to = "rev",
values_to = "mean_gd"
) %>%
mutate(
rev = sub("mean_gd_", "", rev),
rev = factor(rev, levels = c("Full", "Upper", "Lower"))
)
highlight_subjs <- subj_gd_long %>%
distinct(expr_id, subj) %>%
group_by(expr_id) %>%
slice_sample(n = 5) %>%
ungroup()
highlight_subjs
subj_gd_long <- subj_gd_long %>%
mutate(highlight = subj %in% highlight_subjs$subj & expr_id %in% highlight_subjs$expr_id)
ggplot(subj_gd_long, aes(x = rev, y = mean_gd)) +
geom_boxplot(aes(fill = rev), outlier.shape = NA, alpha = 0.8) +
# Light gray lines for all subjects (with jitter on x)
geom_line(data = subj_gd_long, aes(group = subj), size = 0.4, position = position_dodge(0.2), alpha=0.15) +
# geom_line(data = filter(subj_gd_long, highlight), aes(group = subj, color = subj), position = position_dodge(0.2), linewidth=0.8) +
geom_point(data = subj_gd_long, aes(group = subj), size = 1, position = position_dodge(0.2), alpha=0.15) +
# geom_point(data = filter(subj_gd_long, highlight),aes(group = subj, color = subj), size = 2, position = position_dodge(0.2)) +
scale_fill_brewer(palette = "Set2", name = "Visibility") +
# facet_wrap(~ expr_id, scales = "free_y", ncol = 1) +
facet_grid(expr_id~"FPRT" , scales = "free_y") +
labs(
title = NULL,
x = NULL,
y = "FPRT (ms)"
) +
# coord_cartesian(ylim = c(NA, 750)) +
theme_light(base_size = 16) +
theme(
legend.position = "bottom",
strip.background = element_rect(fill = "gray85", color = "black", linewidth = 1),
strip.text = element_text(size = 16, color = "black"),
legend.title = element_text(size = 16),
axis.text.x = element_text(size = 16),
axis.text.y = element_text(size = 16),
axis.title.y = element_text(size = 18)
)
# ggsave("./visualization/zh_char_mean_gd_difference_boxplot_vertical.pdf", width = 3.5, height = 6)
measures = c("total_duration")
languages = c("ZH", "EN")
rev = c("Whole", "Upper", "Lower")
predictors = c("len", "freq", "surp", "renyi")
# predictors = c("surp")
get_d_points = function(df, p) {
values = df[[p]]
dens = density(values, na.rm = TRUE)
data.frame(x = dens$x, y = dens$y)
}
measure_cols <- c("first_duration", "total_duration", "gaze_duration", "go_past_time",
"FPFix", "FPReg", "RegIn_incl", "NFix", "skip")
zhen_long <- zhen_rt %>%
pivot_longer(
cols = all_of(measure_cols),
names_to = "measure",
values_to = "value"
)
View(zhen_long)
density_data = data.frame()
for (m in measures){
for (p in predictors) {
for(l in languages) {
dummy_df = zhen_long %>%
filter(expr_id == l, measure == m) %>%
do({get_d_points(., p)}) %>%
filter(x>0, x<20)
density_data = rbind(density_data, dummy_df %>% mutate(language = l, predictor = p))
}
}
}
density_data <- density_data %>%
mutate(predictor = factor(predictor, levels = c("len", "freq", "surp", "renyi"), labels = c("Word Length", "Zipf Frequency", "Surprisal", "Entropy")),
language = factor(language, levels = c("ZH-CN", "EN")))
View(density_data)
# write.csv(density_data, "./precomputed/density.csv")
density_data <- read.csv("./precomputed/density.csv")
ggplot(density_data, aes(x = x, y = y)) +
geom_line(size = 0.8) +
scale_x_continuous(breaks = c(0, 10, 20), labels = c(0, 10, 20), minor_breaks = NULL) +
facet_grid(language ~ predictor) +
ylab("Density") +
xlab("Predictor Value") +
ggtitle("Density plots for Predictors across Languages") +
theme_minimal() +
theme(
panel.grid.minor = element_blank()
)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
shhh <- suppressPackageStartupMessages
shhh(library(dplyr))
shhh(library(ggplot2))
shhh(library(ggrepel))
shhh(library(forcats))
shhh(library(ggcorrplot))
shhh(library(cowplot))
shhh(library(lme4))
shhh(library(lmerTest))
shhh(library(tidyr))
shhh(library(stringr))
shhh(library(HDInterval))
shhh(library(MASS))
shhh(library(brms))
shhh(library(boot))
shhh(library(purrr))
shhh(library(mgcv))
shhh(library(rsample))
shhh(library(gridExtra))
shhh(library(cmdstanr))
Baseline_lower <- read.csv("/Users/cui/Documents/uzh/PhD/Projects/Re-Veil/Re-Veil/analysis/data/LLM/ZH/char/onestop_baseline/bayesian_baseline_zh_lower_exp_10.0.csv") %>%
dplyr::select(target, entropy) %>%
rename(word = target,
vent = entropy) %>%
mutate(rev = "l",
model = "Baseline")
Baseline_upper <- read.csv("/Users/cui/Documents/uzh/PhD/Projects/Re-Veil/Re-Veil/analysis/data/LLM/ZH/char/onestop_baseline/bayesian_baseline_zh_upper_exp_10.0.csv") %>%
dplyr::select(target, entropy) %>%
rename(word = target,
vent = entropy) %>%
mutate(rev = "u",
model = "Baseline")
Baseline_whole <- read.csv("/Users/cui/Documents/uzh/PhD/Projects/Re-Veil/Re-Veil/analysis/data/LLM/ZH/char/onestop_baseline/bayesian_baseline_zh_whole_exp_10.0.csv") %>%
dplyr::select(target, entropy) %>%
rename(word = target,
vent = entropy) %>%
mutate(rev = "f",
model = "Baseline")
Qwen_zeroshot_lower <- read.csv("/Users/cui/Documents/uzh/PhD/Projects/Re-Veil/Re-Veil/analysis/data/LLM/ZH/char/onestop/lower/zero_shot_onestop_zh_char_lower.csv") %>%
dplyr::select(target, entropy) %>%
rename(word = target,
vent = entropy) %>%
mutate(rev = "l",
model = "Qwen-Zeroshot")
Qwen_zeroshot_upper <- read.csv("/Users/cui/Documents/uzh/PhD/Projects/Re-Veil/Re-Veil/analysis/data/LLM/ZH/char/onestop/upper/zero_shot_onestop_zh_char_upper.csv") %>%
dplyr::select(target, entropy) %>%
rename(word = target,
vent = entropy) %>%
mutate(rev = "u",
model = "Qwen-Zeroshot")
Qwen_zeroshot_whole <- read.csv("/Users/cui/Documents/uzh/PhD/Projects/Re-Veil/Re-Veil/analysis/data/LLM/ZH/char/onestop/whole/zero_shot_onestop_zh_char_whole.csv") %>%
dplyr::select(target, entropy) %>%
rename(word = target,
vent = entropy) %>%
mutate(rev = "f",
model = "Qwen-Zeroshot")
Qwen_finetune_lower <- read.csv("/Users/cui/Documents/uzh/PhD/Projects/Re-Veil/Re-Veil/analysis/data/LLM/ZH/char/onestop/lower/finetuned_onestop_zh_char_lower.csv") %>%
dplyr::select(target, entropy) %>%
rename(word = target,
vent = entropy) %>%
mutate(rev = "l",
model = "Qwen-Finetune")
Qwen_finetune_upper <- read.csv("/Users/cui/Documents/uzh/PhD/Projects/Re-Veil/Re-Veil/analysis/data/LLM/ZH/char/onestop/upper/finetuned_onestop_zh_char_upper.csv") %>%
dplyr::select(target, entropy) %>%
rename(word = target,
vent = entropy) %>%
mutate(rev = "u",
model = "Qwen-Finetune")
Qwen_finetune_whole <- read.csv("/Users/cui/Documents/uzh/PhD/Projects/Re-Veil/Re-Veil/analysis/data/LLM/ZH/char/onestop/whole/finetuned_onestop_zh_char_whole.csv") %>%
dplyr::select(target, entropy) %>%
rename(word = target,
vent = entropy) %>%
mutate(rev = "f",
model = "Qwen-Finetune")
trocr_finetune_lower <- read.csv("/Users/cui/Documents/uzh/PhD/Projects/Re-Veil/Bottom-Up-Information/data/LLM/ZH/char/tr-ocr/zh_onestop_lower.csv") %>%
dplyr::select(target, entropy) %>%
rename(word = target,
vent = entropy) %>%
mutate(rev = "l",
model = "TrOCR")
trocr_finetune_upper <- read.csv("/Users/cui/Documents/uzh/PhD/Projects/Re-Veil/Bottom-Up-Information/data/LLM/ZH/char/tr-ocr/zh_onestop_upper.csv") %>%
dplyr::select(target, entropy) %>%
rename(word = target,
vent = entropy) %>%
mutate(rev = "u",
model = "TrOCR")
trocr_finetune_whole <- read.csv("/Users/cui/Documents/uzh/PhD/Projects/Re-Veil/Bottom-Up-Information/data/LLM/ZH/char/tr-ocr/zh_onestop_whole.csv") %>%
dplyr::select(target, entropy) %>%
rename(word = target,
vent = entropy) %>%
mutate(rev = "f",
model = "TrOCR")
zh_mi <- bind_rows(Baseline_lower, Baseline_upper, Baseline_whole, Qwen_zeroshot_lower, Qwen_zeroshot_upper, Qwen_zeroshot_whole, Qwen_finetune_lower, Qwen_finetune_upper, Qwen_finetune_whole, trocr_finetune_lower, trocr_finetune_upper, trocr_finetune_whole)
zh_mi <- zh_mi %>%
mutate(
uncond_ent = case_when(
model == "TrOCR" ~ 5.59,   # 8.16, if count the trOCR performance as random guessing, 5.25 if take into account freq.
TRUE ~ 5.59
),
vmi = pmax(uncond_ent - vent, 0),
language = "ZH-CN"
)
View(zh_mi)
Baseline_lower <- read.csv("/Users/cui/Documents/uzh/PhD/Projects/Re-Veil/Re-Veil/analysis/data/LLM/EN/words/onestop_baseline/bayesian_baseline_en_lower_exp_10.0.csv") %>%
dplyr::select(target, entropy) %>%
rename(word = target,
vent = entropy) %>%
mutate(rev = "l",
model = "Baseline")
Baseline_upper <- read.csv("/Users/cui/Documents/uzh/PhD/Projects/Re-Veil/Re-Veil/analysis/data/LLM/EN/words/onestop_baseline/bayesian_baseline_en_upper_exp_10.0.csv") %>%
dplyr::select(target, entropy) %>%
rename(word = target,
vent = entropy) %>%
mutate(rev = "u",
model = "Baseline")
Baseline_whole <- read.csv("/Users/cui/Documents/uzh/PhD/Projects/Re-Veil/Re-Veil/analysis/data/LLM/EN/words/onestop_baseline/bayesian_baseline_en_whole_exp_10.0.csv") %>%
dplyr::select(target, entropy) %>%
rename(word = target,
vent = entropy) %>%
mutate(rev = "f",
model = "Baseline")
Qwen_zeroshot_lower <- read.csv("/Users/cui/Documents/uzh/PhD/Projects/Re-Veil/Re-Veil/analysis/data/LLM/EN/words/onestop/lower/zero_shot_en_word_lower.csv") %>%
dplyr::select(target, entropy) %>%
rename(word = target,
vent = entropy) %>%
mutate(rev = "l",
model = "Qwen-Zeroshot")
Qwen_zeroshot_upper <- read.csv("/Users/cui/Documents/uzh/PhD/Projects/Re-Veil/Re-Veil/analysis/data/LLM/EN/words/onestop/upper/zero_shot_en_word_upper.csv") %>%
dplyr::select(target, entropy) %>%
rename(word = target,
vent = entropy) %>%
mutate(rev = "u",
model = "Qwen-Zeroshot")
Qwen_zeroshot_whole <- read.csv("/Users/cui/Documents/uzh/PhD/Projects/Re-Veil/Re-Veil/analysis/data/LLM/EN/words/onestop/whole/zero_shot_en_word_whole.csv") %>%
dplyr::select(target, entropy) %>%
rename(word = target,
vent = entropy) %>%
mutate(rev = "f",
model = "Qwen-Zeroshot")
Qwen_finetune_lower <- read.csv("/Users/cui/Documents/uzh/PhD/Projects/Re-Veil/Re-Veil/analysis/data/LLM/EN/words/onestop/lower/finetune_en_word_lower_best.csv") %>%
dplyr::select(target, entropy) %>%
rename(word = target,
vent = entropy) %>%
mutate(rev = "l",
model = "Qwen-Finetune")
Qwen_finetune_upper <- read.csv("/Users/cui/Documents/uzh/PhD/Projects/Re-Veil/Re-Veil/analysis/data/LLM/EN/words/onestop/upper/finetune_en_word_upper_best.csv") %>%
dplyr::select(target, entropy) %>%
rename(word = target,
vent = entropy) %>%
mutate(rev = "u",
model = "Qwen-Finetune")
Qwen_finetune_whole <- read.csv("/Users/cui/Documents/uzh/PhD/Projects/Re-Veil/Re-Veil/analysis/data/LLM/EN/words/onestop/whole/finetune_en_word_whole_best.csv") %>%
dplyr::select(target, entropy) %>%
rename(word = target,
vent = entropy) %>%
mutate(rev = "f",
model = "Qwen-Finetune")
trocr_finetune_lower <- read.csv("/Users/cui/Documents/uzh/PhD/Projects/Re-Veil/Bottom-Up-Information/data/LLM/EN/words/tr-ocr/en_onestopqa_lower.csv") %>%
dplyr::select(target, entropy) %>%
rename(word = target,
vent = entropy) %>%
mutate(rev = "l",
model = "TrOCR")
trocr_finetune_upper <- read.csv("/Users/cui/Documents/uzh/PhD/Projects/Re-Veil/Bottom-Up-Information/data/LLM/EN/words/tr-ocr/en_onestopqa_upper.csv") %>%
dplyr::select(target, entropy) %>%
rename(word = target,
vent = entropy) %>%
mutate(rev = "u",
model = "TrOCR")
trocr_finetune_whole <- read.csv("/Users/cui/Documents/uzh/PhD/Projects/Re-Veil/Bottom-Up-Information/data/LLM/EN/words/tr-ocr/en_onestopqa_whole.csv") %>%
dplyr::select(target, entropy) %>%
rename(word = target,
vent = entropy) %>%
mutate(rev = "f",
model = "TrOCR")
en_mi <- bind_rows(Baseline_lower, Baseline_upper, Baseline_whole, Qwen_zeroshot_lower, Qwen_zeroshot_upper, Qwen_zeroshot_whole, Qwen_finetune_lower, Qwen_finetune_upper, Qwen_finetune_whole, trocr_finetune_lower, trocr_finetune_upper, trocr_finetune_whole)
en_mi <- en_mi %>%
mutate(
uncond_ent = case_when(
model == "TrOCR" ~ 7.12,
TRUE ~ 7.12,
)) %>%
mutate(vmi = pmax(uncond_ent - vent, 0),
language = "EN")
View(en_mi)
zhen_mi <- rbind(zh_mi, en_mi)
zhen_mi <- zhen_mi %>%
mutate(
rev = factor(rev, levels = c("f", "u", "l"), labels = c("Full", "Upper", "Lower")),
language = factor(language, levels = c("ZH-CN", "EN"), labels = c("ZH", "EN")),
model = factor(model, levels = c("Baseline", "Qwen-Zeroshot", "Qwen-Finetune", "TrOCR"), labels = c("Baseline", "Qwen2.5-Zeroshot", "Qwen2.5-Finetuned", "TransOCR"))
)
View(zhen_mi)
colnames(zhen_mi)
# write.csv(zhen_mi, "/Users/cui/Documents/uzh/PhD/Projects/Re-Veil/Re-Veil/analysis/precomputed/zhen_mi.csv")
plot_df <- zhen_mi #%>%
# filter(model != "Qwen2.5-Zeroshot")
ggplot(plot_df, aes(x = rev, y = vmi, fill = rev)) +
geom_boxplot(
outlier.shape = NA,
color = "gray30",
alpha = 0.8,
linewidth = 0.5
) +
facet_grid(language ~ model, scales = "free_y") +
theme_light(base_size = 13) +
labs(
title = NULL,
x = NULL,
y = "IG of Word ID and Visual Input (nats)",
fill = "Visibility"
) +
scale_fill_brewer(palette = "Set2") +
theme(
strip.background = element_rect(fill = "gray85", color = "black", linewidth = 1),
strip.text = element_text(size = 12, color = "black"),
axis.text.x = element_text(size = 13),
axis.text.y = element_text(size = 13),
legend.text = element_text(size = 13),
legend.title = element_text(size = 13),
legend.position = "bottom",
panel.grid.major.x = element_blank(),
panel.grid.minor = element_blank()
)
ggsave("./visualization/mi_word_visual_pair_inc_zeroshot.pdf", width = 8, height = 4.5)
knitr::opts_chunk$set(echo = TRUE)
